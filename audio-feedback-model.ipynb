{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:48:33.399692Z","iopub.status.busy":"2023-05-16T18:48:33.399396Z","iopub.status.idle":"2023-05-16T18:48:34.432313Z","shell.execute_reply":"2023-05-16T18:48:34.431448Z","shell.execute_reply.started":"2023-05-16T18:48:33.399585Z"},"id":"5VEkNLugWxE_","outputId":"0ca2acb2-9029-4d97-af8d-85991ee6f519","scrolled":true,"trusted":true},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"at2VLWk5oqWB"},"source":["### 1.1 Import all the required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:48:34.434712Z","iopub.status.busy":"2023-05-16T18:48:34.434322Z","iopub.status.idle":"2023-05-16T18:48:41.031263Z","shell.execute_reply":"2023-05-16T18:48:41.030472Z","shell.execute_reply.started":"2023-05-16T18:48:34.434670Z"},"id":"Km6gyfnn5GBX","trusted":true},"outputs":[],"source":["#Import all the required libraries\n","\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from collections import Counter\n","\n","import tensorflow as tf\n","import keras\n","from keras.preprocessing.image import load_img\n","import string\n","import time\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import layers\n","from tensorflow.keras import activations\n","from tensorflow.keras import Input\n","from PIL import Image\n","\n","#used for creating Progress Meters or Progress Bars\n","from tqdm import tqdm\n"]},{"cell_type":"markdown","metadata":{"id":"sfyVqvE-oqWG"},"source":["### 1.2 Install additional libraries required"]},{"cell_type":"markdown","metadata":{"id":"AwfD_GzD32ia"},"source":["We need to install the below libraries before proceeding :\n","\n","**gTTS** - (Google Text-to-Speech)is a Python library and CLI tool to interface with Google Translate text-to-speech API. We will import the gTTS library from the gtts module which can be used for speech translation.\n","\n","**playsound** - The playsound module is a cross platform module that can play audio files. This doesn't have any dependencies, simply install with pip in your virtualenv and run!\n","\n","**display** - Public API for display tools in IPython"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:48:41.032697Z","iopub.status.busy":"2023-05-16T18:48:41.032423Z","iopub.status.idle":"2023-05-16T18:49:11.400140Z","shell.execute_reply":"2023-05-16T18:49:11.399242Z","shell.execute_reply.started":"2023-05-16T18:48:41.032661Z"},"id":"hlMumuYS32Qq","outputId":"637d8e73-3355-4bf4-d51e-5008245d691d","trusted":true},"outputs":[],"source":["# Install additional libraries required\n","\n","!pip install wordcloud\n","!pip install gtts\n","!pip install playsound"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:11.404538Z","iopub.status.busy":"2023-05-16T18:49:11.403856Z","iopub.status.idle":"2023-05-16T18:49:11.459316Z","shell.execute_reply":"2023-05-16T18:49:11.458541Z","shell.execute_reply.started":"2023-05-16T18:49:11.404504Z"},"id":"MUXkkyzu3qNd","outputId":"c8bd3b31-bcbc-4eaf-f31b-f29b144d2386","trusted":true},"outputs":[],"source":["import glob\n","from gtts import gTTS\n","from playsound import playsound\n","from IPython import display\n","\n","import collections\n","import wordcloud\n","from wordcloud import WordCloud, STOPWORDS\n"]},{"cell_type":"markdown","metadata":{"id":"kYiFTZgroqWL"},"source":["### 1.3 Mount drive and import dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:11.461144Z","iopub.status.busy":"2023-05-16T18:49:11.460867Z","iopub.status.idle":"2023-05-16T18:49:17.421095Z","shell.execute_reply":"2023-05-16T18:49:17.420310Z","shell.execute_reply.started":"2023-05-16T18:49:11.461106Z"},"id":"-TJLldZcKn-x","outputId":"062035f8-901b-42c3-8103-d73a7294a514","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{"id":"G_oy951S9lnS"},"source":["### 2.1 Import the dataset and read image & captions into two separate variables"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:17.422819Z","iopub.status.busy":"2023-05-16T18:49:17.422528Z","iopub.status.idle":"2023-05-16T18:49:17.460235Z","shell.execute_reply":"2023-05-16T18:49:17.459402Z","shell.execute_reply.started":"2023-05-16T18:49:17.422781Z"},"id":"_HQhjkDx5GBe","outputId":"1062e884-1913-4e02-da0b-3e342f0a2a25","trusted":true},"outputs":[],"source":["#Import the dataset and read the image into a seperate variable\n","\n","images='/kaggle/input/flickr8k/Images'\n","\n","all_imgs = glob.glob(images + '/*.jpg',recursive=True)\n","print(\"The total images present in the dataset: {}\".format(len(all_imgs)))"]},{"cell_type":"markdown","metadata":{"id":"QjEDFNHA9o_4"},"source":["### 2.2 Visualise both the images & text present in the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:17.462047Z","iopub.status.busy":"2023-05-16T18:49:17.461788Z","iopub.status.idle":"2023-05-16T18:49:18.357988Z","shell.execute_reply":"2023-05-16T18:49:18.356988Z","shell.execute_reply.started":"2023-05-16T18:49:17.462012Z"},"id":"-5JgWYQO5GBf","outputId":"81488627-15bb-4589-96ec-793d512d5586","trusted":true},"outputs":[],"source":["#Visualise both the images & text present in the dataset\n","import imageio\n","\n","#Visualising first 5 images :\n","\n","Display_Images = all_imgs[0:5]\n","figure, axes = plt.subplots(1,5)\n","figure.set_figwidth(20)\n","\n","for ax, image in zip(axes, Display_Images):\n","  ax.imshow(imageio.imread(image), cmap=None)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:18.359898Z","iopub.status.busy":"2023-05-16T18:49:18.359292Z","iopub.status.idle":"2023-05-16T18:49:18.451477Z","shell.execute_reply":"2023-05-16T18:49:18.450879Z","shell.execute_reply.started":"2023-05-16T18:49:18.359844Z"},"id":"T266bICHP8YN","outputId":"fcc79f82-24e0-4643-f8a7-b117264d2dc8","trusted":true},"outputs":[],"source":["# view a random image\n","\n","import random\n","Image.open(all_imgs[random.randrange(40, 60, 3)])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:18.453162Z","iopub.status.busy":"2023-05-16T18:49:18.452745Z","iopub.status.idle":"2023-05-16T18:49:18.506336Z","shell.execute_reply":"2023-05-16T18:49:18.505440Z","shell.execute_reply.started":"2023-05-16T18:49:18.453124Z"},"id":"P_XPOhV75GBh","outputId":"a3d73567-6d39-4af1-e6e0-8d9136ec4c7e","scrolled":true,"trusted":true},"outputs":[],"source":["#Import the dataset and read the text file into a separate variable\n","\n","#txt_file = 'Flickr8K/captions.txt'\n","text_file = '/kaggle/input/flickr8k/captions.txt'\n","\n","def load_doc(filename):\n","    \n","    #your code here\n","    open_file = open(text_file, 'r', encoding='latin-1' ) #returns a file object\n","    text = open_file.read() #reads contents of the file\n","    open_file.close()\n","    #print(text)\n","    \n","    return text\n","\n","doc = load_doc(text_file)\n","print(doc[:300])"]},{"cell_type":"markdown","metadata":{"id":"V9yj_C0R-XlW"},"source":["### 2.3 Create a list which contains all the captions & path\n","\n","#### We will create three lists here and finally combine them to form a dataframe\n","\n","    1. all_img_id = [] to store all the image id \n","    2. all_img_vector = [] to store all the image path \n","    3. annotations = []  to store all the captions "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:18.510047Z","iopub.status.busy":"2023-05-16T18:49:18.509836Z","iopub.status.idle":"2023-05-16T18:49:18.613188Z","shell.execute_reply":"2023-05-16T18:49:18.612474Z","shell.execute_reply.started":"2023-05-16T18:49:18.510019Z"},"id":"tqGD4H6S5GBk","outputId":"f3c214be-20ab-4aaf-ff0e-b8ca2b9a1b3e","trusted":true},"outputs":[],"source":["img_path = '/kaggle/input/flickr8k/Images/'\n","\n","all_img_id = [] #store all the image id here\n","all_img_vector = [] #store all the image path here\n","annotations = [] #store all the captions here\n","\n","with open('/kaggle/input/flickr8k/captions.txt' , 'r') as fo:\n","  next(fo) #to skip the heading\n","  for line in fo :\n","    split_arr = line.split(',')\n","    all_img_id.append(split_arr[0])\n","    annotations.append(split_arr[1].rstrip('\\n.')) #removing out the \\n.\n","    all_img_vector.append(img_path+split_arr[0])\n","\n","df = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['ID','Path', 'Captions']) \n","    \n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:18.615193Z","iopub.status.busy":"2023-05-16T18:49:18.614259Z","iopub.status.idle":"2023-05-16T18:49:18.620546Z","shell.execute_reply":"2023-05-16T18:49:18.619848Z","shell.execute_reply.started":"2023-05-16T18:49:18.615154Z"},"id":"Kj_280Yf9Zkg","outputId":"549031b1-8513-4ddb-a79e-e64257f14eba","trusted":true},"outputs":[],"source":["len (annotations)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:18.622452Z","iopub.status.busy":"2023-05-16T18:49:18.621829Z","iopub.status.idle":"2023-05-16T18:49:18.632399Z","shell.execute_reply":"2023-05-16T18:49:18.631574Z","shell.execute_reply.started":"2023-05-16T18:49:18.622417Z"},"id":"I1TbYSa-WalN","outputId":"25ddf1df-8a37-4598-c1f7-703fd413a404","trusted":true},"outputs":[],"source":["type (annotations)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:18.633876Z","iopub.status.busy":"2023-05-16T18:49:18.633538Z","iopub.status.idle":"2023-05-16T18:49:18.644315Z","shell.execute_reply":"2023-05-16T18:49:18.643551Z","shell.execute_reply.started":"2023-05-16T18:49:18.633833Z"},"id":"v_gaypCD5GBm","outputId":"5fd8920d-45f2-424f-a41b-3317becdb137","trusted":true},"outputs":[],"source":["#check total captions and images present in dataset\n","\n","print(\"Total captions present in the dataset: \"+ str(len(annotations)))\n","print(\"Total images present in the dataset: \" + str(len(all_imgs)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:18.645959Z","iopub.status.busy":"2023-05-16T18:49:18.645560Z","iopub.status.idle":"2023-05-16T18:49:18.832248Z","shell.execute_reply":"2023-05-16T18:49:18.831511Z","shell.execute_reply.started":"2023-05-16T18:49:18.645922Z"},"id":"zvazRH035GBn","outputId":"08d44820-2bfa-4ab0-abc7-4baf133348f1","trusted":true},"outputs":[],"source":["#Create the vocabulary & the counter for the captions\n","#lower() used to ensure same count irrespective of an alphabet's case\n","\n","vocabulary = [word.lower() for line in annotations for word in line.split()]\n","\n","val_count = Counter(vocabulary) \n","val_count"]},{"cell_type":"markdown","metadata":{"id":"dIx0I0YV-iq5"},"source":["### 2.5 Visualise the top 30 occuring words in the captions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:18.834032Z","iopub.status.busy":"2023-05-16T18:49:18.833561Z","iopub.status.idle":"2023-05-16T18:49:19.347102Z","shell.execute_reply":"2023-05-16T18:49:19.346394Z","shell.execute_reply.started":"2023-05-16T18:49:18.833995Z"},"id":"KUeQzrK_5GBo","outputId":"ec16ed25-89b4-4fcf-a964-df049c299b66","trusted":true},"outputs":[],"source":["#Visualise the top 30 occuring words in the captions\n","\n","#write your code here\n","for word, count in val_count.most_common(30):\n","  print(word, \": \", count)\n","\n","lst = val_count.most_common(30)\n","most_common_words_df = pd.DataFrame(lst, columns = ['Word', 'Count'])\n","most_common_words_df.plot.bar(x='Word', y='Count', width=0.6, color='orange', figsize=(15, 10))\n","plt.title(\"Top 30 maximum frequency words\", fontsize = 18, color= 'navy')\n","plt.xlabel(\"Words\", fontsize = 14, color= 'navy')\n","plt.ylabel(\"Count\", fontsize = 14, color= 'navy')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:19.348928Z","iopub.status.busy":"2023-05-16T18:49:19.348481Z","iopub.status.idle":"2023-05-16T18:49:20.805937Z","shell.execute_reply":"2023-05-16T18:49:20.805297Z","shell.execute_reply.started":"2023-05-16T18:49:19.348886Z"},"id":"eNuz3uTj-5Bx","outputId":"c421293f-805c-421e-8909-1152ded2120d","trusted":true},"outputs":[],"source":["## check out the top 30 stopwords with higher frequency\n","\n","wordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(val_count)\n","plt.figure(figsize = (12, 12))\n","plt.imshow(wordcloud)"]},{"cell_type":"markdown","metadata":{"id":"jjgWEUPy_ySJ"},"source":["### Visualize the images and captions together"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:20.807643Z","iopub.status.busy":"2023-05-16T18:49:20.807262Z","iopub.status.idle":"2023-05-16T18:49:21.078259Z","shell.execute_reply":"2023-05-16T18:49:21.077661Z","shell.execute_reply.started":"2023-05-16T18:49:20.807594Z"},"id":"IyZhuUyG_0xN","outputId":"438d29a4-936c-44de-eacf-fe965dce9c19","scrolled":true,"trusted":true},"outputs":[],"source":["def caption_with_img_plot(image_id, frame) :\n","  #get the captions\n","  capt = (\"\\n\" *2).join(frame[frame['ID'] == image_id].Captions.to_list())\n","  fig, ax = plt.subplots()\n","  ax.set_axis_off()\n","  idx = df.ID.to_list().index(image_id)\n","  im =  Image.open(df.Path.iloc[idx])\n","  w, h = im.size[0], im.size[-1]\n","  ax.imshow(im)\n","  ax.text(w+50, h, capt, fontsize = 18, color = 'navy')\n","caption_with_img_plot(df.ID.iloc[8049], df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:21.079973Z","iopub.status.busy":"2023-05-16T18:49:21.079541Z","iopub.status.idle":"2023-05-16T18:49:22.301483Z","shell.execute_reply":"2023-05-16T18:49:22.300773Z","shell.execute_reply.started":"2023-05-16T18:49:21.079932Z"},"id":"deUxtBv0CY4J","outputId":"085d83ca-f108-4206-fefc-31dd75b74239","trusted":true},"outputs":[],"source":["def execute_img_capt(start, end, frame) :\n","  for r in range(start, end) :\n","    caption_with_img_plot(frame.ID.drop_duplicates().iloc[r], frame)\n","\n","execute_img_capt(0, 5, df)  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:22.303161Z","iopub.status.busy":"2023-05-16T18:49:22.302779Z","iopub.status.idle":"2023-05-16T18:49:22.786744Z","shell.execute_reply":"2023-05-16T18:49:22.785975Z","shell.execute_reply.started":"2023-05-16T18:49:22.303122Z"},"id":"GSyzTXMUjJsP","trusted":true},"outputs":[],"source":["#data cleaning\n","rem_punct = str.maketrans('', '', string.punctuation)\n","for r in range(len(annotations)) :\n","  line = annotations[r]\n","  line = line.split()\n","\n","  # converting to lowercase\n","  line = [word.lower() for word in line]\n","\n","  # remove punctuation from each caption and hanging letters\n","  line = [word.translate(rem_punct) for word in line]\n","  line = [word for word in line if len(word) > 1]\n","\n","  # remove numeric values\n","  line = [word for word in line if word.isalpha()]\n","\n","  annotations[r] = ' '.join(line)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:22.788690Z","iopub.status.busy":"2023-05-16T18:49:22.788232Z","iopub.status.idle":"2023-05-16T18:49:22.807474Z","shell.execute_reply":"2023-05-16T18:49:22.806574Z","shell.execute_reply.started":"2023-05-16T18:49:22.788651Z"},"id":"SPQBB9WNikB3","trusted":true},"outputs":[],"source":["#add the <start> & <end> token to all those captions as well\n","annotations = ['<start>' + ' ' + line + ' ' + '<end>' for line in annotations]\n","\n","#Create a list which contains all the path to the images\n","all_img_path = all_img_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:22.809179Z","iopub.status.busy":"2023-05-16T18:49:22.808841Z","iopub.status.idle":"2023-05-16T18:49:22.820280Z","shell.execute_reply":"2023-05-16T18:49:22.819528Z","shell.execute_reply.started":"2023-05-16T18:49:22.809141Z"},"id":"iT3gHUXrikNR","outputId":"8a7b53aa-ab7b-4734-a5c4-a0863a17af6a","trusted":true},"outputs":[],"source":["##list contatining captions for an image\n","annotations[0:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:22.821993Z","iopub.status.busy":"2023-05-16T18:49:22.821599Z","iopub.status.idle":"2023-05-16T18:49:22.829667Z","shell.execute_reply":"2023-05-16T18:49:22.828972Z","shell.execute_reply.started":"2023-05-16T18:49:22.821957Z"},"id":"FXlFvhTS5GBp","trusted":true},"outputs":[],"source":["# create the tokenizer\n","\n","#your code here\n","\n","top_word_cnt = 5000\n","tokenizer = Tokenizer(num_words = top_word_cnt+1, filters= '!\"#$%^&*()_+.,:;-?/~`{}[]|\\=@ ',\n","                      lower = True, char_level = False, \n","                      oov_token = 'UNK')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:22.830983Z","iopub.status.busy":"2023-05-16T18:49:22.830574Z","iopub.status.idle":"2023-05-16T18:49:24.359877Z","shell.execute_reply":"2023-05-16T18:49:24.359106Z","shell.execute_reply.started":"2023-05-16T18:49:22.830820Z"},"id":"zWSduzQ75GBp","trusted":true},"outputs":[],"source":["# Create word-to-index and index-to-word mappings.\n","\n","#your code here\n","\n","tokenizer.fit_on_texts(annotations)\n","\n","#transform each text into a sequence of integers\n","train_seqs = tokenizer.texts_to_sequences(annotations)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:24.363121Z","iopub.status.busy":"2023-05-16T18:49:24.361065Z","iopub.status.idle":"2023-05-16T18:49:24.367396Z","shell.execute_reply":"2023-05-16T18:49:24.366436Z","shell.execute_reply.started":"2023-05-16T18:49:24.363078Z"},"id":"PtcoGgGRowXe","trusted":true},"outputs":[],"source":["# we add PAD token for zero\n","\n","tokenizer.word_index['PAD'] = 0\n","tokenizer.index_word[0] = 'PAD'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:24.369190Z","iopub.status.busy":"2023-05-16T18:49:24.368555Z","iopub.status.idle":"2023-05-16T18:49:24.379781Z","shell.execute_reply":"2023-05-16T18:49:24.378994Z","shell.execute_reply.started":"2023-05-16T18:49:24.369154Z"},"id":"45n08QRYphY9","outputId":"2564e83c-0fb8-4334-f241-52b41315f860","trusted":true},"outputs":[],"source":["print(tokenizer.oov_token)\n","print(tokenizer.index_word[0])"]},{"cell_type":"markdown","metadata":{"id":"l8ABdpoqpj6a"},"source":["#### View index words"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:24.381459Z","iopub.status.busy":"2023-05-16T18:49:24.381107Z","iopub.status.idle":"2023-05-16T18:49:24.428118Z","shell.execute_reply":"2023-05-16T18:49:24.427445Z","shell.execute_reply.started":"2023-05-16T18:49:24.381424Z"},"id":"k453YjB5piwH","outputId":"a606833c-a1e1-423f-a353-6c0b6e0347e1","scrolled":true,"trusted":true},"outputs":[],"source":["tokenizer.index_word"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:24.429854Z","iopub.status.busy":"2023-05-16T18:49:24.429369Z","iopub.status.idle":"2023-05-16T18:49:24.591694Z","shell.execute_reply":"2023-05-16T18:49:24.590571Z","shell.execute_reply.started":"2023-05-16T18:49:24.429809Z"},"id":"UhXgkeld5GBp","outputId":"bd5c8d71-46c4-42b5-9df3-fba5583a409e","trusted":true},"outputs":[],"source":["# Create a word count of your tokenizer to visualize the Top 30 occuring words after text processing\n","\n","#your code here\n","tokenizer_top_words = [word for line in annotations for word in line.split() ]\n","\n","#tokenizer_top_words_count\n","tokenizer_top_words_count = collections.Counter(tokenizer_top_words)\n","tokenizer_top_words_count"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:24.611079Z","iopub.status.busy":"2023-05-16T18:49:24.608638Z","iopub.status.idle":"2023-05-16T18:49:25.151447Z","shell.execute_reply":"2023-05-16T18:49:25.150570Z","shell.execute_reply.started":"2023-05-16T18:49:24.611030Z"},"id":"TDnbKRVdrX8u","outputId":"d76adf4f-b190-4c80-9cdf-03aa8637ed13","trusted":true},"outputs":[],"source":["for word, count in tokenizer_top_words_count.most_common(30) :\n","  print(word, \": \", count)\n","\n","tokens = tokenizer_top_words_count.most_common(30)\n","most_com_words_df = pd.DataFrame(tokens, columns = ['Word', 'Count'])\n","\n","#plot 30 most common words\n","most_common_words_df.plot.bar(x = 'Word', y= 'Count', width=0.8, color = 'indigo', figsize = (17, 10))\n","plt.title('Top 30 common words', fontsize =20, color= 'navy')\n","plt.xlabel('Words', fontsize =14, color= 'navy')\n","plt.ylabel('Counts', fontsize =14, color= 'navy')\n","plt.grid(b=None)"]},{"cell_type":"markdown","metadata":{"id":"WQY_6HARoqWm"},"source":["### Let's view the frequency of words now, post Captions preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:25.153409Z","iopub.status.busy":"2023-05-16T18:49:25.153133Z","iopub.status.idle":"2023-05-16T18:49:26.559133Z","shell.execute_reply":"2023-05-16T18:49:26.558437Z","shell.execute_reply.started":"2023-05-16T18:49:25.153372Z"},"id":"mQC1nDSPtlig","outputId":"42373cb9-76ee-4947-b27b-371e938ca1bd","trusted":true},"outputs":[],"source":["wordcloud_token = WordCloud(width = 1000, height = 500).generate_from_frequencies(tokenizer_top_words_count)\n","plt.figure(figsize = (12, 8))\n","plt.imshow(wordcloud_token)\n","plt.grid(b = None)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:26.561115Z","iopub.status.busy":"2023-05-16T18:49:26.560391Z","iopub.status.idle":"2023-05-16T18:49:26.795834Z","shell.execute_reply":"2023-05-16T18:49:26.795066Z","shell.execute_reply.started":"2023-05-16T18:49:26.561077Z"},"id":"O-VPrP2z5GBq","outputId":"37690f40-5580-4434-9d27-c29d551909fd","trusted":true},"outputs":[],"source":["# Pad each vector to the max_length of the captions ^ store it to a vairable\n","\n","#store the length of all lists\n","train_seqs_len = [len(seq) for seq in train_seqs]\n","\n","#store elements from list with maximum value\n","longest_word_length = max(train_seqs_len)\n","\n","#calculate longest word_length and pads all sequences to equal length as that of the longest.\n","cap_vector= tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding= 'post', maxlen = longest_word_length,\n","                                                          dtype='int32', value=0)\n","\n","print(\"The shape of Caption vector is :\" + str(cap_vector.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:26.797646Z","iopub.status.busy":"2023-05-16T18:49:26.797191Z","iopub.status.idle":"2023-05-16T18:49:26.801744Z","shell.execute_reply":"2023-05-16T18:49:26.801042Z","shell.execute_reply.started":"2023-05-16T18:49:26.797591Z"},"id":"6QP1BmXP-e7W","trusted":true},"outputs":[],"source":["# creating list to store preprocessed images and setting up the Image Shape\n","\n","preprocessed_image = []\n","IMAGE_SHAPE = (299, 299)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:26.803792Z","iopub.status.busy":"2023-05-16T18:49:26.803189Z","iopub.status.idle":"2023-05-16T18:49:26.815462Z","shell.execute_reply":"2023-05-16T18:49:26.814746Z","shell.execute_reply.started":"2023-05-16T18:49:26.803646Z"},"id":"eq3ZEfqi5GBs","outputId":"7936374d-aa21-41de-cf73-18a3ebfaa50b","trusted":true},"outputs":[],"source":["#checking image format \n","\n","tf.keras.backend.image_data_format()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:26.817059Z","iopub.status.busy":"2023-05-16T18:49:26.816718Z","iopub.status.idle":"2023-05-16T18:49:29.713368Z","shell.execute_reply":"2023-05-16T18:49:29.712635Z","shell.execute_reply.started":"2023-05-16T18:49:26.817023Z"},"id":"uahCl7VO5GBs","trusted":true},"outputs":[],"source":["#write your code here for creating the function. This function should return images & their path\n","\n","#write your pre-processing steps here (checking only for the first five images here)\n","for img in all_imgs[0:5] :\n","    img = tf.io.read_file(img, name=None)\n","\n","    # we need to decode jpeg encoded images (here by default channels = 0)\n","    img = tf.image.decode_jpeg(img, channels=0)\n","    img = tf.image.resize(img, (299, 299))\n","    img = tf.keras.applications.inception_v3.preprocess_input(img)\n","\n","    #append preprocessed images to the list\n","    preprocessed_image.append(img)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:29.714956Z","iopub.status.busy":"2023-05-16T18:49:29.714697Z","iopub.status.idle":"2023-05-16T18:49:30.606034Z","shell.execute_reply":"2023-05-16T18:49:30.602799Z","shell.execute_reply.started":"2023-05-16T18:49:29.714919Z"},"id":"RMl1nnpL5GBs","outputId":"12597430-d46d-4fa4-e9cd-c210c69e37b9","trusted":true},"outputs":[],"source":["# checking first five images post preprocessing\n","\n","Display_Images = preprocessed_image[0:5]\n","figure, axes = plt.subplots(1,5)\n","figure.set_figwidth(25)\n","\n","for ax, image in zip(axes, Display_Images) :\n","  print('Shape after resize : ', image.shape)\n","  ax.imshow(image)\n","  ax.grid('off')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:30.607803Z","iopub.status.busy":"2023-05-16T18:49:30.607319Z","iopub.status.idle":"2023-05-16T18:49:30.614063Z","shell.execute_reply":"2023-05-16T18:49:30.613388Z","shell.execute_reply.started":"2023-05-16T18:49:30.607750Z"},"id":"3rOriJpNit1y","trusted":true},"outputs":[],"source":["## write your code here for applying the function to the image path dataset,\n","## such that the transformed dataset should contain images & their path\n","\n","\n","def load_images(image_path) :\n","  img = tf.io.read_file(image_path, name = None)\n","  img = tf.image.decode_jpeg(img, channels=0)\n","  img = tf.image.resize(img, IMAGE_SHAPE)\n","  img = tf.keras.applications.inception_v3.preprocess_input(img)\n","  return img, image_path"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:30.616166Z","iopub.status.busy":"2023-05-16T18:49:30.615442Z","iopub.status.idle":"2023-05-16T18:49:30.648153Z","shell.execute_reply":"2023-05-16T18:49:30.647474Z","shell.execute_reply.started":"2023-05-16T18:49:30.616127Z"},"id":"q0yaT10Wtl1G","outputId":"3fb19489-e49d-4a77-a8ff-8b52675c5d8e","trusted":true},"outputs":[],"source":["all_img_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:30.649877Z","iopub.status.busy":"2023-05-16T18:49:30.649415Z","iopub.status.idle":"2023-05-16T18:49:30.806827Z","shell.execute_reply":"2023-05-16T18:49:30.806116Z","shell.execute_reply.started":"2023-05-16T18:49:30.649837Z"},"id":"fJEa5_4Mtl4c","trusted":true},"outputs":[],"source":["# Map each image full path to the function, in order to preprocess the image\n","\n","## sort the unique paths and store in a list\n","training_list = sorted(set(all_img_vector))\n","\n","#create a new dataset from above training list\n","New_Img = tf.data.Dataset.from_tensor_slices(training_list)\n","\n","#map load_images function across the elements of the new dataset above\n","New_Img = New_Img.map(load_images, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n","#Note  : Here, num_parallel_calls = tf.data.experimental.AUTOTUNE sets the number of parallel calls dynamically.\n","            ## based on the current GPU/CPU\n","\n","#setting a batch size of 64\n","New_Img = New_Img.batch(64, drop_remainder=False)\n","#Note : As we don't want to drop the last batch if it contains less than 64 elements, we set drop_remainder to false"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:30.808233Z","iopub.status.busy":"2023-05-16T18:49:30.807985Z","iopub.status.idle":"2023-05-16T18:49:30.814856Z","shell.execute_reply":"2023-05-16T18:49:30.814181Z","shell.execute_reply.started":"2023-05-16T18:49:30.808198Z"},"id":"MIkgxXo3tl76","outputId":"a6a02645-043b-4917-b2f5-a205c25c54e4","trusted":true},"outputs":[],"source":["New_Img"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:30.816757Z","iopub.status.busy":"2023-05-16T18:49:30.816025Z","iopub.status.idle":"2023-05-16T18:49:30.841981Z","shell.execute_reply":"2023-05-16T18:49:30.841344Z","shell.execute_reply.started":"2023-05-16T18:49:30.816717Z"},"id":"HJDx-qygtl--","trusted":true},"outputs":[],"source":["#Ratio = 80:20 and we will set random state = 42\n","\n","path_train, path_test, caption_train, caption_test = train_test_split(all_img_vector, cap_vector, test_size = 0.2, random_state = 42)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:30.843374Z","iopub.status.busy":"2023-05-16T18:49:30.843057Z","iopub.status.idle":"2023-05-16T18:49:30.851606Z","shell.execute_reply":"2023-05-16T18:49:30.850666Z","shell.execute_reply.started":"2023-05-16T18:49:30.843328Z"},"id":"K6VN5SidtmCc","outputId":"deffd957-c38f-4b2e-f573-cbe59cae2641","trusted":true},"outputs":[],"source":["print(\"Training data for images: \" + str(len(path_train)))\n","print(\"Testing data for images: \" + str(len(path_test)))\n","print(\"Training data for Captions: \" + str(len(caption_train)))\n","print(\"Testing data for Captions: \" + str(len(caption_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:30.853630Z","iopub.status.busy":"2023-05-16T18:49:30.853310Z","iopub.status.idle":"2023-05-16T18:49:33.597145Z","shell.execute_reply":"2023-05-16T18:49:33.596368Z","shell.execute_reply.started":"2023-05-16T18:49:30.853529Z"},"id":"nQyVXhKd5GBt","outputId":"f0f50be9-069f-40ca-8712-a979fc383336","trusted":true},"outputs":[],"source":["image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n","\n","new_input = image_model.input #write code here to get the input of the image_model\n","hidden_layer = image_model.layers[-1].output  #write code here to get the output of the image_model\n","\n","#build the final model using both input & output layer\n","image_features_extract_model = tf.compat.v1.keras.Model(new_input, hidden_layer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:33.598803Z","iopub.status.busy":"2023-05-16T18:49:33.598523Z","iopub.status.idle":"2023-05-16T18:49:33.743976Z","shell.execute_reply":"2023-05-16T18:49:33.743287Z","shell.execute_reply.started":"2023-05-16T18:49:33.598766Z"},"id":"MQjWdIl25GBt","outputId":"5188653a-fa56-4a60-ea51-d8e1ae7910a4","trusted":true},"outputs":[],"source":["# write the code to apply the feature_extraction model to your earlier created dataset which contained images & their respective paths\n","# Once the features are created, you need to reshape them such that feature shape is in order of (batch_size, 8*8, 2048)\n","\n","image_features_extract_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:49:33.745749Z","iopub.status.busy":"2023-05-16T18:49:33.745482Z","iopub.status.idle":"2023-05-16T18:50:30.090467Z","shell.execute_reply":"2023-05-16T18:50:30.089740Z","shell.execute_reply.started":"2023-05-16T18:49:33.745714Z"},"id":"cg8Xvr-BKGZN","outputId":"628b075f-0ba6-42d7-845b-5bcc36b5c1f5","trusted":true},"outputs":[],"source":["# extract features from each image in the dataset\n","\n","img_features = {}\n","for image, image_path in tqdm(New_Img) :\n","  # we are using tqdm for progress bar\n","\n","  # feed images from newly created Dataset above to Inception V3 built above\n","  batch_features = image_features_extract_model(image)\n","  #squeeze out the features in a batch\n","  batch_features_flattened = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n","\n","  for batch_feat, path in zip(batch_features_flattened, image_path) :\n","    feature_path = path.numpy().decode('utf-8')\n","    img_features[feature_path] = batch_feat.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:30.092364Z","iopub.status.busy":"2023-05-16T18:50:30.091734Z","iopub.status.idle":"2023-05-16T18:50:30.128353Z","shell.execute_reply":"2023-05-16T18:50:30.127534Z","shell.execute_reply.started":"2023-05-16T18:50:30.092312Z"},"id":"yU8yWjlDKGib","outputId":"9c5ddfed-fe19-4835-d4c9-75780a239ad8","trusted":true},"outputs":[],"source":["batch_features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:30.130006Z","iopub.status.busy":"2023-05-16T18:50:30.129729Z","iopub.status.idle":"2023-05-16T18:50:30.151822Z","shell.execute_reply":"2023-05-16T18:50:30.151142Z","shell.execute_reply.started":"2023-05-16T18:50:30.129969Z"},"id":"ev8QoOS5KHRP","outputId":"329a0311-f52f-4a44-f9e7-73e24fd3f50c","trusted":true},"outputs":[],"source":["batch_features_flattened"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:30.153271Z","iopub.status.busy":"2023-05-16T18:50:30.152859Z","iopub.status.idle":"2023-05-16T18:50:30.161376Z","shell.execute_reply":"2023-05-16T18:50:30.160527Z","shell.execute_reply.started":"2023-05-16T18:50:30.153236Z"},"id":"kb1mYVNuKHXz","outputId":"54755a4c-1a22-4572-90d8-8d86eefc74ad","trusted":true},"outputs":[],"source":["len(img_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:30.162872Z","iopub.status.busy":"2023-05-16T18:50:30.162532Z","iopub.status.idle":"2023-05-16T18:50:30.171121Z","shell.execute_reply":"2023-05-16T18:50:30.170143Z","shell.execute_reply.started":"2023-05-16T18:50:30.162828Z"},"id":"pYb3DI_SN1Ta","outputId":"9feb71af-58a8-48d7-b14a-4fa0cf535f86","trusted":true},"outputs":[],"source":["batch_feat.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:30.173035Z","iopub.status.busy":"2023-05-16T18:50:30.172762Z","iopub.status.idle":"2023-05-16T18:50:30.191267Z","shell.execute_reply":"2023-05-16T18:50:30.190431Z","shell.execute_reply.started":"2023-05-16T18:50:30.172999Z"},"id":"RSRZzSL4N1jN","outputId":"3db0273d-b543-4b16-ce29-6d61fed6d804","trusted":true},"outputs":[],"source":["#view top five items of img_features dict\n","import more_itertools\n","top_5 = more_itertools.take(5, img_features.items())\n","\n","top_5"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:30.192933Z","iopub.status.busy":"2023-05-16T18:50:30.192663Z","iopub.status.idle":"2023-05-16T18:50:30.197661Z","shell.execute_reply":"2023-05-16T18:50:30.196568Z","shell.execute_reply.started":"2023-05-16T18:50:30.192894Z"},"id":"Sa1oxIaD1-3G","trusted":true},"outputs":[],"source":["#to provide, both images along with the captions as input\n","def map(image_name, caption):\n","    \n","    # your code goes here to create the dataset & transform it\n","    \n","    img_tensor = img_features[image_name.decode('utf-8')]\n","    return img_tensor, caption"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:30.199246Z","iopub.status.busy":"2023-05-16T18:50:30.198990Z","iopub.status.idle":"2023-05-16T18:50:30.209886Z","shell.execute_reply":"2023-05-16T18:50:30.209159Z","shell.execute_reply.started":"2023-05-16T18:50:30.199212Z"},"id":"gglO9Zqh5GBw","scrolled":true,"trusted":true},"outputs":[],"source":["# create a builder function to create dataset which takes in the image path & captions as input\n","# This function should transform the created dataset(img_path,cap) to (features,cap) using the map_func created earlier\n","\n","BUFFER_SIZE = 1000\n","BATCH_SIZE = 64\n","def gen_dataset(img, capt):\n","    \n","    # your code goes here to create the dataset & transform it\n","    \n","    data = tf.data.Dataset.from_tensor_slices((img, capt))\n","    # dataset created using tf.data.Dataset.from_tensor_slices\n","    data = data.map(lambda ele1, ele2 : tf.numpy_function(map, [ele1, ele2], [tf.float32, tf.int32]),\n","                    num_parallel_calls = tf.data.experimental.AUTOTUNE)\n","    \n","     \n","    data = (data.shuffle(BUFFER_SIZE, reshuffle_each_iteration= True).batch(BATCH_SIZE, drop_remainder = False)\n","    .prefetch(tf.data.experimental.AUTOTUNE))\n","    # .prefetch() is used to prepare all upcoming elements, while current elements are being processed\n","    # We set reshuffle_each_iteration set to True in order to ensure different order per epoch\n","    # Also,  drop_remainder is set to False as we don't want to miss out any element if the last batch contains less than 64 elements\n","\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:30.211491Z","iopub.status.busy":"2023-05-16T18:50:30.211236Z","iopub.status.idle":"2023-05-16T18:50:30.437363Z","shell.execute_reply":"2023-05-16T18:50:30.436589Z","shell.execute_reply.started":"2023-05-16T18:50:30.211456Z"},"id":"7AoRRJlL5GBy","trusted":true},"outputs":[],"source":["train_dataset = gen_dataset(path_train,caption_train)\n","test_dataset = gen_dataset(path_test,caption_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:30.439156Z","iopub.status.busy":"2023-05-16T18:50:30.438893Z","iopub.status.idle":"2023-05-16T18:50:31.001173Z","shell.execute_reply":"2023-05-16T18:50:31.000313Z","shell.execute_reply.started":"2023-05-16T18:50:30.439120Z"},"id":"BfdQNzQl5GBz","outputId":"cad71c70-c3b5-44f5-99e9-2d018fc3f98c","trusted":true},"outputs":[],"source":["sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n","print(sample_img_batch.shape)  #(batch_size, 8*8, 2048)\n","print(sample_cap_batch.shape) #(batch_size,max_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.017639Z","iopub.status.busy":"2023-05-16T18:50:31.017392Z","iopub.status.idle":"2023-05-16T18:50:31.023172Z","shell.execute_reply":"2023-05-16T18:50:31.022316Z","shell.execute_reply.started":"2023-05-16T18:50:31.017588Z"},"id":"AIcikdUI5GB0","trusted":true},"outputs":[],"source":["# Setting  parameters\n","\n","embedding_dim = 256 \n","units = 512\n","\n","#top 5,000 words +1\n","vocab_size = 5001\n","train_num_steps = len(path_train) // BATCH_SIZE #len(total train images) // BATCH_SIZE\n","test_num_steps = len(path_test) // BATCH_SIZE  #len(total test images) // BATCH_SIZE\n","\n","max_length = 31\n","feature_shape = batch_feat.shape[1]\n","attention_feature_shape = batch_feat.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.025313Z","iopub.status.busy":"2023-05-16T18:50:31.024687Z","iopub.status.idle":"2023-05-16T18:50:31.035596Z","shell.execute_reply":"2023-05-16T18:50:31.034611Z","shell.execute_reply.started":"2023-05-16T18:50:31.025273Z"},"id":"30MEu5aesjYz","outputId":"f38ae080-ca05-4637-d8c2-c706b179f244","trusted":true},"outputs":[],"source":["tf.compat.v1.reset_default_graph()\n","print(tf.compat.v1.get_default_graph())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.037784Z","iopub.status.busy":"2023-05-16T18:50:31.037087Z","iopub.status.idle":"2023-05-16T18:50:31.046746Z","shell.execute_reply":"2023-05-16T18:50:31.045975Z","shell.execute_reply.started":"2023-05-16T18:50:31.037745Z"},"id":"Evf8gxEr5GB1","trusted":true},"outputs":[],"source":["#Building Encoder using CNN Keras subclassing method\n","\n","class Encoder(Model):\n","    def __init__(self,embed_dim):\n","        super(Encoder, self).__init__()\n","        self.dense = tf.keras.layers.Dense(embed_dim) #build your Dense layer with relu activation\n","        \n","    def call(self, features):\n","        features =  self.dense(features) # extract the features from the image shape: (batch, 8*8, embed_dim)\n","        features =  tf.keras.activations.relu(features, alpha=0.01, max_value=None, threshold=0)\n","        return features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.049179Z","iopub.status.busy":"2023-05-16T18:50:31.048952Z","iopub.status.idle":"2023-05-16T18:50:31.067138Z","shell.execute_reply":"2023-05-16T18:50:31.066332Z","shell.execute_reply.started":"2023-05-16T18:50:31.049153Z"},"id":"C68N2xQo5GB2","trusted":true},"outputs":[],"source":["encoder=Encoder(embedding_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.068844Z","iopub.status.busy":"2023-05-16T18:50:31.068526Z","iopub.status.idle":"2023-05-16T18:50:31.073541Z","shell.execute_reply":"2023-05-16T18:50:31.072687Z","shell.execute_reply.started":"2023-05-16T18:50:31.068783Z"},"id":"TBsI5iVetX7W","trusted":true},"outputs":[],"source":["from keras.utils.vis_utils import plot_model\n","#plot_model(encoder, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.075696Z","iopub.status.busy":"2023-05-16T18:50:31.074847Z","iopub.status.idle":"2023-05-16T18:50:31.085257Z","shell.execute_reply":"2023-05-16T18:50:31.084460Z","shell.execute_reply.started":"2023-05-16T18:50:31.075659Z"},"id":"bee_QVDs5GB3","trusted":true},"outputs":[],"source":["class Attention_model(Model):\n","    def __init__(self, units):\n","        super(Attention_model, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(units) #build your Dense layer\n","        self.W2 = tf.keras.layers.Dense(units) #build your Dense layer\n","        self.V = tf.keras.layers.Dense(1) #build your final Dense layer with unit 1\n","        self.units=units\n","\n","    def call(self, features, hidden):\n","        # features shape: (batch_size, 8*8, embedding_dim)\n","        # hidden shape: (batch_size, hidden_size)\n","\n","        # Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n","        hidden_with_time_axis = hidden[:, tf.newaxis]\n","\n","        # build your score funciton to shape: (batch_size, 8*8, units)\n","        score = tf.keras.activations.tanh(self.W1(features) + self.W2(hidden_with_time_axis))  \n","\n","        # extract your attention weights with shape: (batch_size, 8*8, 1)\n","        attention_weights = tf.keras.activations.softmax(self.V(score), axis=1) \n","\n","        #shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n","        context_vector = attention_weights * features \n","\n","        # reduce the shape to (batch_size, embedding_dim)\n","        context_vector = tf.reduce_sum(context_vector, axis=1)  \n","        \n","        return context_vector, attention_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.087708Z","iopub.status.busy":"2023-05-16T18:50:31.087026Z","iopub.status.idle":"2023-05-16T18:50:31.318128Z","shell.execute_reply":"2023-05-16T18:50:31.317273Z","shell.execute_reply.started":"2023-05-16T18:50:31.087670Z"},"id":"ol38QKyR5GB4","trusted":true},"outputs":[],"source":["class Decoder(Model):\n","    def __init__(self, embed_dim, units, vocab_size):\n","        super(Decoder, self).__init__()\n","        self.units=units\n","        self.attention = Attention_model(self.units) #iniitalise your Attention model with units\n","        self.embed = tf.keras.layers.Embedding(vocab_size, embed_dim) #build your Embedding layer\n","        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n","        self.d1 = tf.keras.layers.Dense(self.units) #build your Dense layer\n","        self.d2 = tf.keras.layers.Dense(vocab_size) #build your Dense layer\n","        \n","\n","    def call(self,x,features, hidden):\n","        context_vector, attention_weights = self.attention(features, hidden) #create your context vector & attention weights from attention model\n","        embed = self.embed(x) # embed your input to shape: (batch_size, 1, embedding_dim)\n","        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis = -1) # Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n","        output,state = self.gru(embed) # Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n","        output = self.d1(output)\n","        output = tf.reshape(output, (-1, output.shape[2])) # shape : (batch_size * max_length, hidden_size)\n","        output = self.d2(output) # shape : (batch_size * max_length, vocab_size)\n","        \n","        return output, state, attention_weights\n","    \n","    def init_state(self, batch_size):\n","        return tf.zeros((batch_size, self.units))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.319983Z","iopub.status.busy":"2023-05-16T18:50:31.319435Z","iopub.status.idle":"2023-05-16T18:50:31.347931Z","shell.execute_reply":"2023-05-16T18:50:31.347268Z","shell.execute_reply.started":"2023-05-16T18:50:31.319945Z"},"id":"2H-Qegu05GB5","trusted":true},"outputs":[],"source":["decoder=Decoder(embedding_dim, units, vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.349215Z","iopub.status.busy":"2023-05-16T18:50:31.348973Z","iopub.status.idle":"2023-05-16T18:50:31.627045Z","shell.execute_reply":"2023-05-16T18:50:31.626262Z","shell.execute_reply.started":"2023-05-16T18:50:31.349179Z"},"id":"CYg60cuC5GB5","outputId":"a2faae6c-be6e-4cdf-cf29-f1d969426d5f","trusted":true},"outputs":[],"source":["features=encoder(sample_img_batch)\n","\n","hidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\n","dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n","\n","predictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\n","print('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\n","print('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\n","print('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.628909Z","iopub.status.busy":"2023-05-16T18:50:31.628452Z","iopub.status.idle":"2023-05-16T18:50:31.634334Z","shell.execute_reply":"2023-05-16T18:50:31.633595Z","shell.execute_reply.started":"2023-05-16T18:50:31.628869Z"},"id":"79nJFjv05GB6","trusted":true},"outputs":[],"source":["optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)  #define the optimizer\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = tf.keras.losses.Reduction.NONE) #define your loss object"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.636643Z","iopub.status.busy":"2023-05-16T18:50:31.635779Z","iopub.status.idle":"2023-05-16T18:50:31.646674Z","shell.execute_reply":"2023-05-16T18:50:31.645983Z","shell.execute_reply.started":"2023-05-16T18:50:31.636590Z"},"id":"2F2niqM35GCG","trusted":true},"outputs":[],"source":["def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    #loss is getting multiplied with mask to get an ideal shape\n","    \n","    return tf.reduce_mean(loss_)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.648498Z","iopub.status.busy":"2023-05-16T18:50:31.648134Z","iopub.status.idle":"2023-05-16T18:50:31.658899Z","shell.execute_reply":"2023-05-16T18:50:31.658161Z","shell.execute_reply.started":"2023-05-16T18:50:31.648463Z"},"id":"UiQUd7305GCI","trusted":true},"outputs":[],"source":["checkpoint_path = \"Flickr8K/checkpoint1\"\n","ckpt = tf.train.Checkpoint(encoder=encoder,\n","                           decoder=decoder,\n","                           optimizer = optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.660558Z","iopub.status.busy":"2023-05-16T18:50:31.660302Z","iopub.status.idle":"2023-05-16T18:50:31.669770Z","shell.execute_reply":"2023-05-16T18:50:31.669017Z","shell.execute_reply.started":"2023-05-16T18:50:31.660524Z"},"id":"bZ6yPbrG5GCJ","trusted":true},"outputs":[],"source":["start_epoch = 0\n","if ckpt_manager.latest_checkpoint:\n","    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.671635Z","iopub.status.busy":"2023-05-16T18:50:31.671234Z","iopub.status.idle":"2023-05-16T18:50:31.682824Z","shell.execute_reply":"2023-05-16T18:50:31.682048Z","shell.execute_reply.started":"2023-05-16T18:50:31.671574Z"},"id":"Q2pig7yS5GCK","trusted":true},"outputs":[],"source":["@tf.function\n","def train_step(img_tensor, target):\n","    loss = 0\n","    hidden = decoder.init_state(batch_size=target.shape[0])\n","    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n","    \n","    with tf.GradientTape() as tape:\n","        #write your code here to do the training steps\n","        encoder_op = encoder(img_tensor)\n","\n","        #apply teacher forcing by passing target word as next input to the decoder\n","        for r in range(1, target.shape[1]) :\n","          #pass encoder_op to decoder\n","          predictions, hidden, _ = decoder(dec_input, encoder_op, hidden)\n","          loss = loss + loss_function(target[:, r], predictions) \n","          dec_input = tf.expand_dims(target[:, r], 1)  \n","\n","    avg_loss = (loss/ int(target.shape[1])) #avg loss per batch\n","    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n","    grad = tape.gradient (loss, trainable_vars) # calculating gradient wrt each trainable var\n","\n","    #we will now compute the gradients and apply it to the optimizer while backpropagating\n","    optimizer.apply_gradients(zip(grad, trainable_vars))\n","\n","    return loss, avg_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.684497Z","iopub.status.busy":"2023-05-16T18:50:31.684176Z","iopub.status.idle":"2023-05-16T18:50:31.697141Z","shell.execute_reply":"2023-05-16T18:50:31.696360Z","shell.execute_reply.started":"2023-05-16T18:50:31.684439Z"},"id":"-CDnlwOH5GCL","trusted":true},"outputs":[],"source":["@tf.function\n","def test_step(img_tensor, target):\n","    loss = 0\n","    \n","    #write your code here to do the testing steps\n","    hidden = decoder.init_state(batch_size = target.shape[0])\n","\n","    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n","    with tf.GradientTape() as tape:\n","      encoder_op = encoder(img_tensor)\n","\n","      #apply teacher forcing again\n","      for r in range(1, target.shape[1]) :\n","        #pass encoder_op to decoder\n","        predictions, hidden, _ = decoder(dec_input, encoder_op, hidden)\n","        loss = loss + loss_function(target[:, r], predictions)\n","\n","        dec_input = tf.expand_dims(target[: , r], 1)\n","\n","    avg_loss = (loss/ int(target.shape[1])) #avg loss per batch\n","    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n","    grad = tape.gradient (loss, trainable_vars) # calculating gradient wrt each trainable var\n","\n","    #we will now compute the gradients and apply it to the optimizer while backpropagating\n","    optimizer.apply_gradients(zip(grad, trainable_vars))                      \n","\n","\n","    return loss, avg_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.698770Z","iopub.status.busy":"2023-05-16T18:50:31.698484Z","iopub.status.idle":"2023-05-16T18:50:31.710447Z","shell.execute_reply":"2023-05-16T18:50:31.709662Z","shell.execute_reply.started":"2023-05-16T18:50:31.698736Z"},"id":"MY25F3fo5GCL","trusted":true},"outputs":[],"source":["def test_loss_cal(test_dataset):\n","    total_loss = 0\n","\n","    #write your code to get the average loss result on your test data\n","    for (batch, (img_tensor, target)) in enumerate(test_dataset) :\n","      batch_loss, t_loss = test_step(img_tensor, target)\n","      total_loss = total_loss + t_loss\n","      avg_test_loss = total_loss/ test_num_steps\n","\n","    return avg_test_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T18:50:31.712219Z","iopub.status.busy":"2023-05-16T18:50:31.711864Z","iopub.status.idle":"2023-05-16T19:10:02.482869Z","shell.execute_reply":"2023-05-16T19:10:02.482088Z","shell.execute_reply.started":"2023-05-16T18:50:31.712184Z"},"id":"j5LGvDQ15GCL","outputId":"d30413a3-8964-4ec6-a3e5-3029a4ecf8ee","trusted":true},"outputs":[],"source":["loss_plot = []\n","test_loss_plot = []\n","EPOCHS = 15\n","\n","best_test_loss=100\n","for epoch in tqdm(range(0, EPOCHS)):\n","    start = time.time()\n","    total_loss = 0\n","\n","    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n","        batch_loss, t_loss = train_step(img_tensor, target)\n","        total_loss += t_loss\n","        avg_train_loss=total_loss / train_num_steps\n","        \n","    loss_plot.append(avg_train_loss)    \n","    test_loss = test_loss_cal(test_dataset)\n","    test_loss_plot.append(test_loss)\n","    \n","    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n","    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","    \n","    if test_loss < best_test_loss:\n","        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n","        best_test_loss = test_loss\n","        ckpt_manager.save()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:10:02.485114Z","iopub.status.busy":"2023-05-16T19:10:02.484581Z","iopub.status.idle":"2023-05-16T19:10:02.739004Z","shell.execute_reply":"2023-05-16T19:10:02.738295Z","shell.execute_reply.started":"2023-05-16T19:10:02.485074Z"},"id":"uVVSCYfk5GCM","outputId":"77904d58-15a8-443f-dff9-4fdea1e7dc2f","trusted":true},"outputs":[],"source":["from matplotlib.pyplot import figure\n","figure(figsize=(12, 8))\n","plt.plot(loss_plot, color='orange', label = 'training_loss_plot')\n","plt.plot(test_loss_plot, color='green', label = 'test_loss_plot')\n","plt.xlabel('Epochs', fontsize = 15, color = 'red')\n","plt.ylabel('Loss', fontsize = 15, color = 'red')\n","plt.title('Loss Plot', fontsize = 20, color = 'red')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:10:02.740469Z","iopub.status.busy":"2023-05-16T19:10:02.740215Z","iopub.status.idle":"2023-05-16T19:10:02.752066Z","shell.execute_reply":"2023-05-16T19:10:02.751282Z","shell.execute_reply.started":"2023-05-16T19:10:02.740430Z"},"id":"sKBTxbtT5GCN","trusted":true},"outputs":[],"source":["def evaluate(image):\n","    attention_plot = np.zeros((max_length, attention_feature_shape))\n","\n","    hidden = decoder.init_state(batch_size=1)\n","\n","    temp_input = tf.expand_dims(load_images(image)[0], 0) #process the input image to desired format before extracting features\n","    img_tensor_val = image_features_extract_model(temp_input) # Extract features using our feature extraction model\n","    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n","\n","    features = encoder (img_tensor_val) # extract the features by passing the input to encoder\n","\n","    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n","    result = []\n","\n","    for i in range(max_length):\n","        predictions, hidden, attention_weights = decoder(dec_input, features, hidden) # get the output from decoder\n","\n","        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n","\n","        predicted_id = tf.argmax(predictions[0]).numpy() #extract the predicted id(embedded value) which carries the max value\n","        #map the id to the word from tokenizer and append the value to the result list\n","        result.append (tokenizer.index_word[predicted_id])\n","\n","        if tokenizer.index_word[predicted_id] == '<end>':\n","            return result, attention_plot,predictions\n","\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    attention_plot = attention_plot[:len(result), :]\n","    return result, attention_plot,predictions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:10:02.754159Z","iopub.status.busy":"2023-05-16T19:10:02.753457Z","iopub.status.idle":"2023-05-16T19:10:02.766932Z","shell.execute_reply":"2023-05-16T19:10:02.766138Z","shell.execute_reply.started":"2023-05-16T19:10:02.754118Z"},"id":"X35rme2SlanC","trusted":true},"outputs":[],"source":["def plot_attention_map (caption, weights, image) :\n","\n","  fig = plt.figure(figsize = (10, 10))\n","  temp_img = np.array(Image.open(image))\n","\n","  cap_len = len(caption)\n","  for cap in range(cap_len) :\n","    weights_img = np.reshape(weights[cap], (8,8))\n","    wweights_img = np.array(Image.fromarray(weights_img).resize((224,224), Image.LANCZOS))\n","\n","    ax = fig.add_subplot(cap_len//2, cap_len//2, cap+1)\n","    ax.set_title(caption[cap], fontsize = 14, color = 'red')\n","\n","    img = ax.imshow(temp_img)\n","\n","    ax.imshow(weights_img, cmap='gist_heat', alpha=0.6, extent=img.get_extent())\n","    ax.axis('off')\n","  plt.subplots_adjust(hspace=0.2, wspace=0.2)\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:10:02.768489Z","iopub.status.busy":"2023-05-16T19:10:02.768119Z","iopub.status.idle":"2023-05-16T19:10:03.288754Z","shell.execute_reply":"2023-05-16T19:10:03.288003Z","shell.execute_reply.started":"2023-05-16T19:10:02.768451Z"},"id":"W04Ya3Pk5GCO","trusted":true},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:10:03.290173Z","iopub.status.busy":"2023-05-16T19:10:03.289915Z","iopub.status.idle":"2023-05-16T19:10:03.296161Z","shell.execute_reply":"2023-05-16T19:10:03.295460Z","shell.execute_reply.started":"2023-05-16T19:10:03.290138Z"},"id":"ptbJAVg15GCP","trusted":true},"outputs":[],"source":["def filt_text(text):\n","    filt=['<start>','<unk>','<end>'] \n","    temp= text.split()\n","    [temp.remove(j) for k in filt for j in temp if k==j]\n","    text=' '.join(temp)\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:10:03.298299Z","iopub.status.busy":"2023-05-16T19:10:03.297764Z","iopub.status.idle":"2023-05-16T19:10:03.307937Z","shell.execute_reply":"2023-05-16T19:10:03.307061Z","shell.execute_reply.started":"2023-05-16T19:10:03.298259Z"},"id":"l9nVZC-1JvAc","trusted":true},"outputs":[],"source":["image_test = path_test.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMVOLGCi5GCP","trusted":true},"outputs":[],"source":["def pred_caption_audio(random, autoplay=False, weights=(0.5, 0.5, 0, 0)) :\n","\n","    cap_test_data = caption_test.copy()\n","    #rid = np.random.randint(0, random)\n","    #test_image = image_test[rid]\n","    test_image = '/kaggle/input/new-data/WhatsApp Image 2023-05-14 at 11.11.43 PM.jpeg'\n","    real_caption = '<start> Boy standing <end>'\n","\n","    #real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test_data[rid] if i not in [0]])\n","    result, attention_plot, pred_test = evaluate(test_image)\n","\n","\n","    real_caption=filt_text(real_caption)      \n","\n","\n","    pred_caption=' '.join(result).rsplit(' ', 1)[0]\n","\n","\n","    real_appn = []\n","    real_appn.append(real_caption.split())\n","    reference = real_appn\n","    candidate = pred_caption.split()\n","\n","    score = sentence_bleu(reference, candidate, weights=weights)#set your weights\n","    print(f\"BELU score: {score*100}\")\n","    print ('Real Caption:', real_caption)\n","    print ('Prediction Caption:', pred_caption)\n","    plot_attention_map(result, attention_plot, test_image)\n","\n","    # we will make use of Google Text to Speech API (online), which will convert the caption to audio\n","    speech = gTTS('Predicted Caption : ' + pred_caption, lang = 'en', slow = False)\n","    speech.save('voice.mp3')\n","    audio_file = 'voice.mp3'\n","\n","    display.display(display.Audio(audio_file, rate = None, autoplay = autoplay))\n","\n","    return test_image\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:27:11.405110Z","iopub.status.busy":"2023-05-16T19:27:11.404795Z","iopub.status.idle":"2023-05-16T19:27:13.686783Z","shell.execute_reply":"2023-05-16T19:27:13.685796Z","shell.execute_reply.started":"2023-05-16T19:27:11.405077Z"},"trusted":true},"outputs":[],"source":["test_image = pred_caption_audio(len('/kaggle/input/new-data/WhatsApp Image 2023-05-14 at 11.11.43 PM.jpeg'), True, weights = (0.5, 0.25, 0, 0))\n","Image.open(test_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:22:51.463981Z","iopub.status.busy":"2023-05-16T19:22:51.463694Z","iopub.status.idle":"2023-05-16T19:22:52.621053Z","shell.execute_reply":"2023-05-16T19:22:52.620420Z","shell.execute_reply.started":"2023-05-16T19:22:51.463950Z"},"id":"d-o4rxHVKU2F","outputId":"309384f9-3d1b-4fd6-d6a6-31ae17cb0094","trusted":true},"outputs":[],"source":["test_image = pred_caption_audio(len(image_test), True, weights = (0.5, 0.25, 0, 0))\n","Image.open(test_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:22:58.355372Z","iopub.status.busy":"2023-05-16T19:22:58.355082Z","iopub.status.idle":"2023-05-16T19:22:59.829347Z","shell.execute_reply":"2023-05-16T19:22:59.828661Z","shell.execute_reply.started":"2023-05-16T19:22:58.355339Z"},"id":"o0JdNHBRLLlz","outputId":"f641c9e9-2682-4dff-cf40-ef927f0d9466","trusted":true},"outputs":[],"source":["test_image = pred_caption_audio(len(image_test), True, weights = (0.5, 0.25, 0, 0))\n","Image.open(test_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:23:08.806811Z","iopub.status.busy":"2023-05-16T19:23:08.806053Z","iopub.status.idle":"2023-05-16T19:23:09.710661Z","shell.execute_reply":"2023-05-16T19:23:09.709879Z","shell.execute_reply.started":"2023-05-16T19:23:08.806774Z"},"id":"X7xo5YY_LQmA","outputId":"1bbb6406-43e7-4cb2-94ef-676ccf8a4e1c","trusted":true},"outputs":[],"source":["test_image = pred_caption_audio(len(image_test), True, weights = (0.5, 0.25, 0, 0))\n","Image.open(test_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:23:15.343295Z","iopub.status.busy":"2023-05-16T19:23:15.343006Z","iopub.status.idle":"2023-05-16T19:23:16.510642Z","shell.execute_reply":"2023-05-16T19:23:16.509994Z","shell.execute_reply.started":"2023-05-16T19:23:15.343262Z"},"id":"CYHX_RtCjifI","outputId":"1dc6f447-43b9-43f0-b6f3-930309f5e41c","trusted":true},"outputs":[],"source":["test_image = pred_caption_audio(len(image_test), True, weights = (0.35, 0.25, 0, 0))\n","Image.open(test_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:23:25.827364Z","iopub.status.busy":"2023-05-16T19:23:25.826577Z","iopub.status.idle":"2023-05-16T19:23:27.381045Z","shell.execute_reply":"2023-05-16T19:23:27.380429Z","shell.execute_reply.started":"2023-05-16T19:23:25.827325Z"},"id":"yklC1K0akhdH","outputId":"d7ae1861-62d7-47fb-9ca6-cdd1ac1840fc","trusted":true},"outputs":[],"source":["test_image = pred_caption_audio(len(image_test), True, weights = (0.5, 0.5, 0, 0))\n","Image.open(test_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:23:34.184916Z","iopub.status.busy":"2023-05-16T19:23:34.184625Z","iopub.status.idle":"2023-05-16T19:23:35.285996Z","shell.execute_reply":"2023-05-16T19:23:35.284889Z","shell.execute_reply.started":"2023-05-16T19:23:34.184882Z"},"trusted":true},"outputs":[],"source":["test_image = pred_caption_audio(len(image_test), True, weights = (0.25, 0.5, 0, 0))\n","Image.open(test_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:23:43.560598Z","iopub.status.busy":"2023-05-16T19:23:43.560313Z","iopub.status.idle":"2023-05-16T19:23:44.552998Z","shell.execute_reply":"2023-05-16T19:23:44.552189Z","shell.execute_reply.started":"2023-05-16T19:23:43.560566Z"},"trusted":true},"outputs":[],"source":["test_image = pred_caption_audio(len(image_test), True, weights = (0.25, 0.25, 0, 0))\n","Image.open(test_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T19:23:52.855405Z","iopub.status.busy":"2023-05-16T19:23:52.855083Z","iopub.status.idle":"2023-05-16T19:23:53.865159Z","shell.execute_reply":"2023-05-16T19:23:53.864463Z","shell.execute_reply.started":"2023-05-16T19:23:52.855360Z"},"trusted":true},"outputs":[],"source":["test_image = pred_caption_audio(len(image_test), True, weights = (0.25, 0.5, 0, 0))\n","Image.open(test_image)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":623289,"sourceId":1111676,"sourceType":"datasetVersion"},{"datasetId":3267180,"sourceId":5683024,"sourceType":"datasetVersion"},{"datasetId":3268154,"sourceId":5684563,"sourceType":"datasetVersion"},{"datasetId":3272582,"sourceId":5691761,"sourceType":"datasetVersion"}],"dockerImageVersionId":30171,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
